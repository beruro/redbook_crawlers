import requests
import json
import re
import pandas as pd
import os
from datetime import datetime
import time
import random

# ä»ç¯å¢ƒå˜é‡è·å– Cookie
def get_cookies():
    # cookies_str = os.environ.get('XIAOHONGSHU_COOKIES')
    # if cookies_str:
    #     try:
    #         return json.loads(cookies_str)
    #     except:
    #         # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•è§£æä¸ºå­—å…¸
    #         cookies = {}
    #         for item in cookies_str.split(';'):
    #             if '=' in item:
    #                 key, value = item.strip().split('=', 1)
    #                 cookies[key] = value
    #         return cookies
    
    # å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ Cookie
    return {
    "a1": "19363212f4acl9fm3846czee2men7mekzmbtw3zgg30000352410",
    "webId": "03a0d588d937b6c7f6f8a607e08e2415",
    "gid": "yjqKqJy42fuiyjqKqJyJi6qu40S1ji6qY4KSDkkuddJ6d9q8IW6dTE888q2J4y88fYq0dWKq",
    "customerClientId": "195177604463666",
    "abRequestId": "03a0d588d937b6c7f6f8a607e08e2415",
    "x-user-id-creator.xiaohongshu.com": "5aae83144eacab32f473510e",
    "web_session": "030037a07a83c92536a83f2eae204a75bf449d",
    "x-user-id-pgy.xiaohongshu.com": "666fcc2de200000000000001",
    "xsecappid": "ratlin",
    "acw_tc": "0a0d0e0317528178280124508e484abf9feaa6595bfcf6f61bffd5e7cbf1de",
    "websectiga": "f47eda31ec99545da40c2f731f0630efd2b0959e1dd10d5fedac3dce0bd1e04d",
    "sec_poison_id": "00f05050-2c2e-48da-a3ef-5eb9a67449be",
    "customer-sso-sid": "68c517528295397851909491ozwwokr3m1tmeecj",
    "solar.beaker.session.id": "AT-68c517528295397756569461xsm0irgry5lopioj",
    "access-token-pgy.xiaohongshu.com": "customer.pgy.AT-68c517528295397756569461xsm0irgry5lopioj",
    "access-token-pgy.beta.xiaohongshu.com": "customer.pgy.AT-68c517528295397756569461xsm0irgry5lopioj",
    "loadts": "1752817900223"
}

# åˆå§‹åŒ–å…¨å±€cookieså˜é‡ï¼Œå…è®¸åŠ¨æ€æ›´æ–°
cookies = get_cookies()

def scrape_xiaohongshu_blogger(user_id):
    # åŸºç¡€URL
    user_info_url = f"https://pgy.xiaohongshu.com/api/solar/cooperator/user/blogger/{user_id}"
    data_summary_url = f"https://pgy.xiaohongshu.com/api/pgy/kol/data/data_summary?userId={user_id}&business=1"
    
    # ä¸¤ä¸ªè¯·æ±‚å…±ç”¨çš„è¯·æ±‚å¤´
    headers = {
        'accept': 'application/json, text/plain, */*',
        'accept-language': 'zh-CN,zh;q=0.9',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
        'referer': f'https://pgy.xiaohongshu.com/solar/pre-trade/blogger-detail/{user_id}',
        'sec-ch-ua': '"Google Chrome";v="135", "Not-A.Brand";v="8", "Chromium";v="135"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
    }
    
    # å‘é€ç¬¬ä¸€ä¸ªè¯·æ±‚è·å–ç”¨æˆ·ä¿¡æ¯
    try:
        response1 = requests.get(user_info_url, headers=headers, cookies=cookies)
        response1.raise_for_status()
        user_data = response1.json()
        
        # å‘é€ç¬¬äºŒä¸ªè¯·æ±‚è·å–æ•°æ®æ‘˜è¦
        response2 = requests.get(data_summary_url, headers=headers, cookies=cookies)
        response2.raise_for_status()
        summary_data = response2.json()
        
        # æå–æ‰€éœ€ä¿¡æ¯
        result = {
            "è¾¾äººåç§°": user_data.get("data", {}).get("name", "æœªçŸ¥"),
            "id": user_data.get("data", {}).get("redId", "æœªçŸ¥"),
            "ä¸»é¡µé“¾æ¥": f"https://www.xiaohongshu.com/user/profile/{user_id}",
            "ç²‰ä¸æ•°(W)": round(user_data.get("data", {}).get("fansCount", 0) / 10000, 1),
        }
        
        # å¤„ç†èµè—æ•°ï¼Œå¯èƒ½æ˜¯æ•´æ•°æˆ–å­—ç¬¦ä¸²
        like_collect = user_data.get("data", {}).get("likeCollectCountInfo", 0)
        if isinstance(like_collect, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²(ä¾‹å¦‚"80.0w"æˆ–"80.0ä¸‡")ï¼Œåˆ™å»æ‰å•ä½å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            like_collect = float(like_collect.replace("w", "").replace("ä¸‡", ""))
        elif isinstance(like_collect, (int, float)):
            # å¦‚æœå·²ç»æ˜¯æ•°å­—ï¼Œåˆ™è½¬æ¢ä¸ºä¸‡ä¸ºå•ä½
            like_collect = like_collect / 10000
        
        result["èµè—æ•°(W)"] = round(like_collect, 1)
        result["äº’åŠ¨ä¸­ä½æ•°"] = summary_data.get("data", {}).get("mEngagementNum", "N/A")
        result["é˜…è¯»ä¸­ä½æ•°"] = summary_data.get("data", {}).get("readMedian", "N/A")
        result["æ›å…‰ä¸­ä½æ•°"] = summary_data.get("data", {}).get("mAccumImpNum", "N/A")
        result["å¤–æº¢è¿›åº—ä¸­ä½æ•°"] = summary_data.get("data", {}).get("mCpuvNum", "N/A")
        

        return result
        
    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å‡ºé”™: {e}")
        return None
    except json.JSONDecodeError:
        print("è§£æJSONå“åº”å‡ºé”™")
        return None
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        return None

def extract_user_id_from_url(url):
    """ä» URL ä¸­æå–ç”¨æˆ· ID"""
    # æ”¯æŒå¤šç§ URL æ ¼å¼
    patterns = [
        r'blogger-detail/([a-zA-Z0-9]+)',  # åŸå§‹æ ¼å¼
        r'user/profile/([a-zA-Z0-9]+)',    # ç”¨æˆ·ä¸»é¡µæ ¼å¼
        r'xiaohongshu\.com/user/profile/([a-zA-Z0-9]+)',  # å®Œæ•´ç”¨æˆ·ä¸»é¡µ URL
        r'xiaohongshu\.com/discovery/item/([a-zA-Z0-9]+)'  # ç¬”è®° URL
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    return None

def process_urls(urls):
    """å¤„ç†å¤šä¸ªURLå¹¶è¿”å›ç»“æœåˆ—è¡¨"""
    all_data = []
    results = []
    
    print(f"ğŸ” process_urls è°ƒè¯•ä¿¡æ¯:")
    print(f"  è¾“å…¥URLæ•°é‡: {len(urls)}")
    print(f"  å‰3ä¸ªURL: {urls[:3]}")
    
    # å°†å¤šä¸ªURLåˆ†æ‰¹å¤„ç†
    batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªURL
    
    for i, url in enumerate(urls):
        url = url.strip()
        if not url:
            continue
            
        # ä»URLä¸­æå–ç”¨æˆ·ID
        user_id = extract_user_id_from_url(url)
        
        if user_id:
            log_message = f"æ­£åœ¨å¤„ç†URL {i+1}/{len(urls)}: {url}, ç”¨æˆ·ID: {user_id}"
            results.append({"status": "processing", "message": log_message})
            
            try:
                # ä½¿ç”¨æ›´åˆç†çš„éšæœºå»¶æ—¶
                delay = random.uniform(2, 5)  # 2-5ç§’çš„éšæœºå»¶æ—¶
                results.append({"status": "info", "message": f"ç­‰å¾… {delay:.1f} ç§’..."})
                time.sleep(delay)
                
                # çˆ¬å–åšä¸»æ•°æ®
                result = scrape_xiaohongshu_blogger(user_id)
                
                if result:
                    all_data.append(result)
                    print(f"âœ… æˆåŠŸçˆ¬å–ç”¨æˆ· {user_id}: {result.get('è¾¾äººåç§°', 'æœªçŸ¥')}")
                    results.append({"status": "success", "message": f"æˆåŠŸè·å– {result['è¾¾äººåç§°']} çš„æ•°æ®"})
                else:
                    print(f"âŒ çˆ¬å–ç”¨æˆ· {user_id} å¤±è´¥")
                    results.append({"status": "error", "message": f"è·å–ç”¨æˆ· {user_id} æ•°æ®å¤±è´¥"})
            except Exception as e:
                results.append({"status": "error", "message": f"å¤„ç†ç”¨æˆ· {user_id} æ—¶å‡ºé”™: {str(e)}"})
            
            # æ¯å¤„ç†ä¸€æ‰¹URLï¼Œé¢å¤–ä¼‘æ¯è¾ƒçŸ­æ—¶é—´
            if (i + 1) % batch_size == 0 and i < len(urls) - 1:
                pause_time = random.uniform(5, 10)  # 5-10ç§’çš„ä¼‘æ¯æ—¶é—´
                results.append({"status": "info", "message": f"æ‰¹é‡å¤„ç†æš‚åœï¼Œä¼‘æ¯ {pause_time:.1f} ç§’..."})
                time.sleep(pause_time)
        else:
            results.append({"status": "error", "message": f"æ— æ³•ä»URLä¸­æå–ç”¨æˆ·ID: {url}"})
    
    print(f"ğŸ“Š process_urls å®Œæˆç»Ÿè®¡:")
    print(f"  æˆåŠŸçˆ¬å–æ•°æ®æ¡æ•°: {len(all_data)}")
    print(f"  è¿”å›ç»“æœæ¡æ•°: {len(results)}")
    if all_data:
        print(f"  æ•°æ®é¢„è§ˆ: {all_data[:2]}")
    
    return all_data, results

def save_to_excel(data_list):
    """ä¿å­˜æ•°æ®åˆ°Excelå¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    if not data_list:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return None
        
    print(f"ğŸ“Š å‡†å¤‡ä¿å­˜ {len(data_list)} æ¡æ•°æ®åˆ°Excel")
    
    # æ·»åŠ æ—¶é—´æˆ³åˆ°æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"å°çº¢ä¹¦è¾¾äººæ•°æ®_{timestamp}.xlsx"
    
    # å°è¯•å¤šä¸ªä¿å­˜è·¯å¾„ï¼ˆé€‚é…äº‘å¹³å°ï¼‰
    save_paths = [
        os.path.join("results", filename),      # resultsç›®å½•
        filename,                               # å½“å‰ç›®å½•
        os.path.join("/tmp", filename),         # ä¸´æ—¶ç›®å½•ï¼ˆäº‘å¹³å°ï¼‰
        os.path.join(os.getcwd(), filename),    # æ˜ç¡®çš„å½“å‰å·¥ä½œç›®å½•
        os.path.join("/app", filename),         # åº”ç”¨æ ¹ç›®å½•
        os.path.join(os.path.expanduser("~"), filename)  # ç”¨æˆ·ç›®å½•
    ]
    
    # åˆ›å»ºDataFrame
    try:
        df = pd.DataFrame(data_list)
        print(f"âœ… DataFrameåˆ›å»ºæˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")
        
        # æ‰“å°æ•°æ®é¢„è§ˆ
        print("ğŸ“‹ æ•°æ®é¢„è§ˆ:")
        for i, item in enumerate(data_list[:2]):  # åªæ˜¾ç¤ºå‰2æ¡
            print(f"  {i+1}. {item.get('è¾¾äººåç§°', 'æœªçŸ¥')}: {item.get('ç²‰ä¸æ•°(W)', 'N/A')}Wç²‰ä¸")
            
    except Exception as e:
        print(f"âŒ åˆ›å»ºDataFrameå¤±è´¥: {e}")
        return None
    
    saved_path = None
    save_errors = []
    
    # å°è¯•ä¿å­˜åˆ°ä¸åŒè·¯å¾„
    for i, save_path in enumerate(save_paths):
        try:
            print(f"ğŸ”„ å°è¯•ä¿å­˜è·¯å¾„ {i+1}/{len(save_paths)}: {save_path}")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            directory = os.path.dirname(save_path)
            if directory and not os.path.exists(directory):
                print(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")
                os.makedirs(directory, exist_ok=True)
            
            # ä¿å­˜åˆ°Excel
            df.to_excel(save_path, index=False, engine='openpyxl')
            print(f"ğŸ’¾ Excelä¿å­˜æ“ä½œå®Œæˆ")
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
            if os.path.exists(save_path):
                file_size = os.path.getsize(save_path)
                print(f"âœ… æ–‡ä»¶éªŒè¯æˆåŠŸ: {save_path}")
                print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                
                if file_size > 1024:  # è‡³å°‘1KB
                    saved_path = save_path
                    print(f"ğŸ‰ æ–‡ä»¶æˆåŠŸä¿å­˜åˆ°: {save_path}")
                    break
                else:
                    print(f"âš ï¸ æ–‡ä»¶å¤ªå° ({file_size} å­—èŠ‚)ï¼Œå¯èƒ½ä¿å­˜å¤±è´¥")
                    save_errors.append(f"{save_path}: æ–‡ä»¶å¤ªå°")
            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {save_path}")
                save_errors.append(f"{save_path}: æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            error_msg = f"ä¿å­˜åˆ° {save_path} å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            save_errors.append(error_msg)
            continue
    
    if saved_path:
        print(f"ğŸ† æœ€ç»ˆæˆåŠŸä¿å­˜: {saved_path}")
        return os.path.basename(saved_path)  # è¿”å›æ–‡ä»¶å
    else:
        print("ğŸ’¥ æ‰€æœ‰ä¿å­˜è·¯å¾„éƒ½å¤±è´¥äº†!")
        print("ğŸ“‹ é”™è¯¯è¯¦æƒ…:")
        for error in save_errors:
            print(f"  - {error}")
        
        # å°è¯•è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯
        print("ğŸ” ç³»ç»Ÿä¿¡æ¯:")
        print(f"  å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        print(f"  å¯å†™æ€§æµ‹è¯•...")
        
        for path in ["/tmp", ".", "/app"]:
            try:
                test_file = os.path.join(path, "test_write.tmp")
                with open(test_file, 'w') as f:
                    f.write("test")
                if os.path.exists(test_file):
                    os.remove(test_file)
                    print(f"  âœ… {path} å¯å†™")
                else:
                    print(f"  âŒ {path} å†™å…¥å¤±è´¥")
            except Exception as e:
                print(f"  âŒ {path} ä¸å¯å†™: {e}")
        
        return None